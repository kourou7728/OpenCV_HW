{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6. 設定值處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1 : 什麼是 Threshold 二值化處理\n",
    "### Thresh `門檻, 閾`\n",
    "> threshold 是`門檻值`的意思，OpenCV 提供的 threshold 工具包裡面有影像門檻值的功能，當畫素值高於門檻值時，我們給這個畫素賦予一個新值（可能是白色），否則我們給它賦予另一種顏色（也許是黑色）。這個函式就是 cv2.threshold()\n",
    "\n",
    "> 圖像的二值化就是將圖像上的圖元點的`灰度值`設置為 0 或 255，這樣將使整個圖像呈現出明顯的黑白效果。在數位影像處理中，二值圖像佔有非常重要的地位，圖像的二值化使圖像中資料量大為減少，從而能凸顯出`目標的輪廓`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ret, dst = cv2.threshold(src, thresh, maxval, type)$\n",
    "> * ret = thresh\n",
    "> * dst : 輸出圖 \n",
    "> * src : 輸入圖，只能輸入單通道影像，通常來說為灰度圖\n",
    "> * thresh ：閾, 門檻值\n",
    "> * maxval ：當畫素值超過了門檻值（或者小於門檻值，根據type來決定），所賦予的值\n",
    "> * type :\n",
    "\n",
    "|語法 type        |值 |說明                                                         |\n",
    "|-----------------|--|-------------------------------------------------------------|\n",
    "|THRESH_BINARY    |0 |即二值化，將大於門檻值的灰度值設為最大灰度值，小於門檻值的值設為0  |\n",
    "|THRESH_BINARY_INV|1 |將大於門檻值的灰度值設為0，其他值設為最大灰度值                  |\n",
    "|THRESH_TRUNC     |2 |將大於門檻值的灰度值設為門檻值，小於門檻值的值保持不變。(灰黑)    |\n",
    "|THRESH_TOZERO    |3 |將小於門檻值的灰度值設為0，大於門檻值的值保持不變。(黑灰白對比)   |\n",
    "|THRESH_TOZERO_INV|4 |將大於門檻值的灰度值設為0，小於門檻值的值保持不變。(黑灰強烈)     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img src=\"./image/thresh4.jpg\"  style='width:100%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img=np.random.randint(100, 150, size=[5, 5], dtype=np.uint8)\n",
    "print(f'img : \\n{img}\\n')\n",
    "\n",
    "thd, t1 = cv2.threshold(img, 125, 245, cv2.THRESH_BINARY)   # try 125\n",
    "print(f'thd : {thd}\\n\\n'\n",
    "      f't1 :\\n{t1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.resize(cv2.imread('./image/thresh.jpg', 0), (300,200))\n",
    "# img = cv2.resize(cv2.imread('./image/lenaColor.png', 0),(400, 300))\n",
    "    \n",
    "ret, thresh1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)  # 1=255, 0=0\n",
    "ret, thresh2 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "ret, thresh3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC)   # 1=127 Thresh, 0=value\n",
    "ret, thresh4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)  # 1=value,  0=0\n",
    "ret, thresh5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "\n",
    "titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "print(f'ret : {ret}')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "for idx, (t, i) in enumerate(zip(titles, images)):\n",
    "    plt.subplot(2, 3, idx + 1),  plt.imshow(i, 'gray')\n",
    "    plt.setp(plt.title(t), color='w')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2: 自我調節設定\n",
    "### threshold adaptive ``局部`` 自我調節設定\n",
    "> 自我調整門檻,閾值二值化函數根據圖片一小塊區域的值來計算對應區域的門檻, 閾值，從而得到也許更為合適的圖片。\n",
    "> * thresh_type ： 門檻, 閾值的計算方法，包含以下2種類型：\n",
    ">> * cv2.ADAPTIVE_THRESH_MEAN_C : 鄰域`面積的平均值`\n",
    ">> * cv2.ADAPTIVE_THRESH_GAUSSIAN_C : 高斯窗口的鄰域值的`加權和`\n",
    "> * Block Size ： 圖片中分塊的大小\n",
    "> * C ：閾值計算方法中的常數項src−類的對像表示源（輸入）圖像。offset ( thresh - c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=np.random.randint(0, 255, size=[5, 5], dtype=np.uint8)\n",
    "\n",
    "print(f'img :\\n{img}\\n')\n",
    "\n",
    "t1, thd = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)   # try 127 → 125\n",
    "print(f'threshHold : {t1}\\n\\n'\n",
    "      f'thd :\\n{thd}\\n')\n",
    "\n",
    "Ad_thd_mean = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 3)\n",
    "print(f'Ad_thd_mean :\\n{Ad_thd_mean}\\n')\n",
    "\n",
    "Ad_thd_gauss = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 3)\n",
    "print(f'Ad_thd_gauss :\\n{Ad_thd_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('./image/mybaby.jpg', 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "ret, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)  # 1 : 255, 0 : 0\n",
    "cv2.imshow('Thresh hold 127, 255', thresh)\n",
    "\n",
    "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 3)\n",
    "cv2.imshow('adaptive / Mean Thresh', thresh)\n",
    "\n",
    "thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 3, 3)\n",
    "cv2.imshow('adaptive /  Gaussian Thresh', thresh)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3 : otsu 處理\n",
    "> #### Otsu 演算法假設這副圖片由前景色和背景色組成，通過統計學方法（`最大類間方差`）選取一個閾值，將前景和背景盡可能分開。也就是說這還是一個全域門檻, 閾值問題。\n",
    "> ### threshold otsu : 是一種自動門檻值決定法則\n",
    ">><img src=\"./image/otsu.png\"  style='width:90%'>\n",
    "\n",
    "### Otsu 過程 ：\n",
    "> * 計算圖像長條圖\n",
    "> * 設定一門檻, 閾值，把長條圖強度大於門檻, 閾值的圖元分成一組，把小於閾值的圖元分成另外一組\n",
    "> * 分別計算兩組內的偏移數，``並把偏移數相加``\n",
    "> * 把 0 ~ 255 依照順序多為閾值，重複 1-3 的步驟，``直到得到最大偏移數``，其所對應的值即為結果門檻, 閾值。\n",
    "\n",
    "> https://www.cnblogs.com/gezhuangzhuang/p/10295181.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=np.random.randint(0, 256, size=[5, 5], dtype=np.uint8)\n",
    "print(f'img : \\n{img}\\n')\n",
    "\n",
    "th2, img2 = cv2.threshold(img, 0, 255,  cv2.THRESH_OTSU)  # type\n",
    "print(f'THRESH_OTSU th2 : {th2}\\n\\n'\n",
    "      f'img2 :\\n{img2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cv2 module\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# image = cv2.imread('./image/mybaby.jpg', 0)\n",
    "image = cv2.imread('./image/lenaColor.png', 0)\n",
    "\n",
    "cv2.imshow('original', image)\n",
    "\n",
    "ret, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)  # 1 : 255, 0 : 0\n",
    "cv2.imshow('Thresh hold : 127', thresh)\n",
    "\n",
    "# Otsu threshold\n",
    "th2, img2 = cv2.threshold(image, 0, 255,  cv2.THRESH_OTSU)\n",
    "print(f\"Otsu's threshold : {th2}\")\n",
    "cv2.imshow(f'Otsu : {th2}', img2)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(image.ravel(), 256)   # 畫直方圖\n",
    "plt.axvline(x=th2, color='r', lw=1)\n",
    "# print(plt.ylim()[1])\n",
    "plt.text(th2+5, plt.ylim()[1]*.9, f'Otsu : {th2}', fontsize=10, color='r')\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## threshold simple ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('./image/mybaby.jpg', 0)\n",
    "blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "T, thresh = cv2.threshold(blurred, 155, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('Threshold Binary', thresh)\n",
    "\n",
    "T, threshInv = cv2.threshold(blurred, 155, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imshow('Threshold Binary Inv.', threshInv)\n",
    "\n",
    "cv2.imshow('bitwise_and', cv2.bitwise_and(image, image, mask=threshInv))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 7. 邊緣檢測\n",
    "## 7-1 : 什麼是邊緣檢測\n",
    "\n",
    "> Edge detection 邊緣偵測是 Computer Vision 中最重要的步驟，它讓電腦能準確的抓取圖中的物體，這項技術運用到相當複雜的數學運算，以檢查影像中各像素點的顏色變化程度來區分邊界。有了邊緣之後，這些交錯的線段中會有所謂的`輪廓`，而這也是電腦取得影像中物件的依據。\n",
    "\n",
    "> openCV 提供三種邊緣檢測方式來處理 `Sobel、Canny 及 Laplacian`，這些技術皆是使用 `灰階` 的影像，基於每個像素灰度的不同，利用不同物體在其邊界處會`有明顯的邊緣特徵來分辨`。這三種方法皆使用了`一維甚至於二維的微分`，嚴格來說，若依其使用技術原理的不同可分為兩種：\n",
    "\n",
    "> * 而 Sobel 和 Canny 使用的則是 Gradient methods（梯度原理），它是透過計算像素光度的`一階導數差異`（detect changes in the first derivative of intensity）來進行邊緣檢測。\n",
    "> * Laplacian 原稱為 Laplacian method，透過計算零交越點上光度的`二階導數`（detect zero crossings of the second derivative on intensity changes）\n",
    "\n",
    "> ### 離散導數\n",
    "><img src=\"./image/sobel_dev.jpg\"  style='width:100%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2 : Sobel、Scharr、Laplacian\n",
    "\n",
    "### 絕對值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=np.random.randint(-256, 256, size=[4, 5], dtype=np.int16)   # –32768 ~ 32767,  0 ~ 65535\n",
    "rst=cv2.convertScaleAbs(img)                                    # 絕對值\n",
    "print(f'img =\\n{img}\\n\\n'\n",
    "      f'rst =\\n{rst}\\n\\n'\n",
    "      f'np.abs()\\n{np.abs(img)}')    # numpy 也可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel : 是一種過濾器，只是其是帶有`方向`的\n",
    "> 結合了`高斯平滑與微分運算`的結合方法, 所以它的抗噪聲能力很強\n",
    "\n",
    "> Sobel 與 Canny 兩者雖然使用`相同的底層技術`，但執行方式有些差異。Sobel 以簡單的卷積過濾器（convolutional filter）偵測圖像上`水平及縱向`光度的改變，以`加權平均方式計算`各點的數值來決定邊緣。\n",
    "\n",
    "> 光影變化，這光影變化在術語上就是所謂`「梯度」(gradient)`<br>\n",
    "><img src=\"./image/sobelHow.jpg\"  style='width:90%'></img>\n",
    "><img src=\"./image/laplacian01.jpg\"  style='width:90%'></img><br>\n",
    "> 加重中間像素的權重<br>\n",
    "\n",
    ">> $ Gx = \\begin{pmatrix} -1&0&+1\\\\-2&0&+2\\\\-1&0&+1 \\end{pmatrix},  \n",
    " Gy = \\begin{pmatrix} -1&-2&-1\\\\0&0&0\\\\+1&+2&+1 \\end{pmatrix}$<br>\n",
    " \n",
    ">> $ f(x, y) = \\sqrt{Gx^2 + Gy^2}$<br>\n",
    ">> $ f(x, y) = |Gx| + |Gy|$<br>\n",
    ">> $ f(x, y) = max[|Gx|, |Gy|]$\n",
    "\n",
    "<!-- ><img src=\".\\image\\ppt\\sobel.png\"  style='height:200px; width:500px'></img> -->\n",
    "\n",
    "><img src=\"./image/sobel_formula.jpg\"  style='width:90%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dst = cv2.Sobel(src, ddepth, dx, dy[, dst[, ksize[, scale[, delta[, borderType]]]]])\n",
    "\n",
    "> * src : 需要處理的影像\n",
    "> * ddepth : 影像的深度，`-1 表示採用的是與原影像相同的深度`。\n",
    ">> * CV_8U ：8-bit unsigned integers ( 0~255 )\n",
    ">> * CV_8S ：8-bit signed integers ( -128~127 )\n",
    ">> * CV_16U ：16-bit unsigned integers ( 0~65535 ) → 大小相當於short\n",
    ">> * CV_16S ：16-bit signed integers ( -32768~32767 ) → 大小相當於short\n",
    ">> * CV_32S ：32-bit signed integers ( -2147483648~2147483647 ) → 大小相當於long\n",
    ">> * CV_32F ：32-bit ﬂoating-point numbers\n",
    ">> * CV_64F ：64-bit ﬂoating-point numbers\n",
    "\n",
    "### 注意 : `當運算中有出現負值, 影像深度 (ddepth) 必須加大`\n",
    "> 目標影像的深度必須大於等於原影像的深度；\n",
    "\n",
    "|src.depth()      |ddepth                |\n",
    "|-----------------|----------------------|\n",
    "| CV_8U           |CV_16S, CV_32F, CV_64F|\n",
    "| CV_16U / CV_16S |CV_32F, CV_64F        |\n",
    "| CV_32F          |CV_32F, CV_64F        |\n",
    "| CV_64F          |CV_64F                |\n",
    "\n",
    "> * dx 和 dy 表示的是求導的階數，`0 表示這個方向上沒有求導`，一般為 0、1。\n",
    "> * ksize : 是 Sobel 運算元的大小，`必須為 1、3、5、7`。\n",
    "> * scale : 是縮放導數的比例常數，預設情況下`沒有`伸縮係數\n",
    "> * delta : 是一個可選的增量，將會加到最終的dst中，同樣，預設情況下`沒有`額外的值加到dst中\n",
    "> * borderType : 是判斷影像邊界的模式。這個引數預設值為 cv2.BORDER_DEFAULT。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddepth = -1, uint8 單邊解譯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((7, 7), dtype=np.uint8)\n",
    "img[1:6, 1:6] = 10  # padding problem = 10\n",
    "print(f'img :\\n{img}\\n')\n",
    "\n",
    "sobelx = cv2.Sobel(img, -1, 1, 0, ksize=-1)  # ddepth = -1, dx=1, dy=0\n",
    "sobely = cv2.Sobel(img, -1, 0, 1, ksize=-1)  # ddepth = -1, dx=0, dy=1\n",
    "sobelxy =  cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)  # dst = src1*alpha + src2*beta + gamma;\n",
    "\n",
    "print(f'sobelx uint8 :\\n{sobelx}\\n\\n'\n",
    "      f'sobely uint8 :\\n{sobely}\\n\\n'\n",
    "      f'sobelxy :\\n{sobelxy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddepth = cv2.CV_64F 雙邊解譯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((7, 7), dtype=np.uint8)\n",
    "img[1:6, 1:6] = 10\n",
    "print(f'img :\\n{img}\\n')\n",
    "\n",
    "sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0)   # ddepth = cv2.CV64V\n",
    "sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1)   # ddepth = cv2.CV64V\n",
    "\n",
    "sobelx = cv2.convertScaleAbs(sobelx)        # 絕對值, 轉換為cv2.CV_8U  \n",
    "sobely = cv2.convertScaleAbs(sobely)        # 絕對值, 轉換為cv2.CV_8U  \n",
    "sobelxy = cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)  \n",
    "\n",
    "print(f'sobelx CV_64F :\\n{sobelx}\\n\\n'\n",
    "      f'sobely CV_64F :\\n{sobely}\\n\\n'\n",
    "      f'sobelxy :\\n{sobelxy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddepth = -1, 原圖是 CV_8U, 因 kernel 有負值, CV_8U ``無法處理負值``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只處理了一邊\n",
    "import cv2\n",
    "o = cv2.imread('./image/sobel.bmp')\n",
    "# o = cv2.imread('./image/contour.png')\n",
    "sobelx = cv2.Sobel(o, -1, 1, 0, ksize=-1)   # o, ddepth=-1, dx=1, dy=0, ksize = -1(default)\n",
    "sobely = cv2.Sobel(o, -1, 0, 1, ksize=-1)   # o, ddepth=-1, dx=0, dy=1, ksize = -1(default)\n",
    "# fg1 = cv2.bitwise_and(sobelx, sobely)\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('dx, CV_U8', sobelx)\n",
    "cv2.imshow('dy, CV_U8', sobely)\n",
    "# cv2.imshow('y',fg1)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如果影像的深度資訊是 np.uint8, 負值全部會變成 0, 這樣會丟掉邊界資訊。\n",
    "### 如果想要兩種邊界都檢測到, 最好的辦法就是將輸出資料型別設定更高, `cv2.CV_16S, cv2.CV_64F`等等。然後取`絕對值`, 轉換為cv2.CV_8U\n",
    "### dx = 1 or dy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "# o = cv2.imread('./image/contour.png')\n",
    "o = cv2.imread('./image/road1.jpg')\n",
    "\n",
    "sobelx = cv2.Sobel(o, cv2.CV_16S, 1, 0)     # ddepth = cv2.CV_16S, dx=1, dy=0, \n",
    "# sobelx = cv2.Sobel(o, cv2.CV_64F, 1, 0)   # ddepth = cv2.CV64V\n",
    "sobelx = cv2.convertScaleAbs(sobelx)        # 絕對值, 轉換為cv2.CV_8U  \n",
    "\n",
    "sobely = cv2.Sobel(o, cv2.CV_64F, 0, 1)     # ddepth = cv2.CV_64V, dx=0, dy=1, \n",
    "sobely = cv2.convertScaleAbs(sobely)        # 絕對值, 轉換為 cv2.CV_8U \n",
    "\n",
    "sobelxy_or = cv2.bitwise_or(sobelx, sobely)\n",
    "sobelxy_and = cv2.bitwise_and(sobelx, sobely)\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('x CV_16S', sobelx)\n",
    "cv2.imshow('y CV_16S', sobely)\n",
    "cv2.imshow('xy_or', sobelxy_or)\n",
    "cv2.imshow('xy_and', sobelxy_and)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dx=1 and dy=1 方向,  get intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "o = cv2.imread('./image/contour.png')\n",
    "\n",
    "# sobelxy=cv2.Sobel(o, cv2.CV_16S, 1, 1)  # dx=1, dy=1\n",
    "sobelxy=cv2.Sobel(o, cv2.CV_64F, 1, 1)\n",
    "sobelxy=cv2.convertScaleAbs(sobelxy) \n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('xy', sobelxy)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "o = cv2.imread('./image/contour.png')\n",
    "\n",
    "sobelx = cv2.Sobel(o, cv2.CV_64F, 1, 0)\n",
    "sobely = cv2.Sobel(o, cv2.CV_64F, 0, 1)\n",
    "\n",
    "sobelx = cv2.convertScaleAbs(sobelx)   # 轉回 uint8 \n",
    "sobely = cv2.convertScaleAbs(sobely)  \n",
    "\n",
    "sobelxy =  cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)  \n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('xy', sobelxy)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight 0.5, 0.5 vs. dx=1, dy=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "o = cv2.imread('./image/lenaColor.png')\n",
    "sobelx = cv2.Sobel(o, cv2.CV_64F, 1, 0)\n",
    "sobely = cv2.Sobel(o, cv2.CV_64F, 0, 1)\n",
    "# print(sobelx[0])\n",
    "\n",
    "sobelx = cv2.convertScaleAbs(sobelx)          # 轉回 uint8\n",
    "sobely = cv2.convertScaleAbs(sobely)  \n",
    "sobelxy = cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)  \n",
    "#=====================================================\n",
    "sobelxy11 = cv2.Sobel(o, cv2.CV_64F, 1, 1)\n",
    "sobelxy11 = cv2.convertScaleAbs(sobelxy11) \n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('x', sobelx)\n",
    "cv2.imshow('y', sobely)\n",
    "cv2.imshow('x_add_y', sobelxy)\n",
    "cv2.imshow('xy11', sobelxy11)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scharr 濾波器\n",
    "> Scharr 濾波器是對 Sobel 運算元差異性的增強，兩者之間的在檢測圖像邊緣的原理和使用方式上相同。而 Scharr 濾波器的主要思路是通過將模版中的`權重係數放大來增大圖元值間的差異`。\n",
    "\n",
    "> Scharr 濾波器，也是計算 x 或 y 方向上的圖像差分，在 OpenCV 中主要是配合 Sobel 運算元的運算而存在的，其濾波器的濾波係數如下：\n",
    "\n",
    ">### kernel :\n",
    ">> $ scharrX = \\begin{pmatrix} +3&0&-3\\\\+10&0&-10\\\\+3&0&-3 \\end{pmatrix},  \n",
    " scharrY = \\begin{pmatrix} +3&+10&+3\\\\0&0&0\\\\-3&-10&-3 \\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scharr : dx , dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "o = cv2.imread('./image/lenaColor.png')  # 微分更敏感\n",
    "\n",
    "# scharrx = cv2.Scharr(o, cv2.CV_64F, 1, 0)    # try dx=1, dy=1\n",
    "# scharry = cv2.Scharr(o, cv2.CV_64F, 0, 1)\n",
    "\n",
    "scharrx = cv2.Scharr(o, cv2.CV_16S, 1, 0)    # try dx=1, dy=1\n",
    "scharry = cv2.Scharr(o, cv2.CV_16S, 0, 1)\n",
    "\n",
    "# scharrx = cv2.Scharr(o, -1, 1, 0)    # try dx=1, dy=1\n",
    "# scharry = cv2.Scharr(o, -1, 0, 1)\n",
    "\n",
    "scharrx = cv2.convertScaleAbs(scharrx)   # 轉回uint8 \n",
    "scharry = cv2.convertScaleAbs(scharry)   # 轉回uint8 \n",
    "\n",
    "cv2.imshow('original',o)\n",
    "cv2.imshow('x', scharrx)\n",
    "cv2.imshow('y', scharry)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scharr : addWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "o = cv2.imread('./image/lenaColor.png')  # 微分更敏感\n",
    "\n",
    "scharrx = cv2.Scharr(o, cv2.CV_64F, 1, 0)    # try dx=1, dy=1\n",
    "scharry = cv2.Scharr(o, cv2.CV_64F, 0, 1)\n",
    "\n",
    "# scharrx = cv2.Scharr(o, cv2.CV_16S, 1, 0)    # try dx=1, dy=1\n",
    "# scharry = cv2.Scharr(o, cv2.CV_16S, 0, 1)\n",
    "\n",
    "scharrx = cv2.convertScaleAbs(scharrx)   # 轉回uint8 \n",
    "scharry = cv2.convertScaleAbs(scharry)  \n",
    "\n",
    "scharrxy =  cv2.addWeighted(scharrx, 0.5, scharry, 0.5, 0)  \n",
    "cv2.imshow('original',o)\n",
    "cv2.imshow('xy',scharrxy)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel vs. Scharr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "o = cv2.imread('./image/lenaColor.png')\n",
    "\n",
    "# =========== Sobel ======================\n",
    "sobelx = cv2.Sobel(o, cv2.CV_64F, 1, 0, ksize=3)\n",
    "sobely = cv2.Sobel(o, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "sobelx = cv2.convertScaleAbs(sobelx)   # 轉回 Uint8 \n",
    "sobely = cv2.convertScaleAbs(sobely)  \n",
    "sobelxy =  cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0) \n",
    "\n",
    "# ========== Scharr ======================\n",
    "scharrx = cv2.Scharr(o, cv2.CV_64F,1,0)\n",
    "scharry = cv2.Scharr(o, cv2.CV_64F,0,1)\n",
    "\n",
    "scharrx = cv2.convertScaleAbs(scharrx)   # 轉回 Uint8 \n",
    "scharry = cv2.convertScaleAbs(scharry)  \n",
    "scharrxy =  cv2.addWeighted(scharrx, 0.5,scharry, 0.5, 0) \n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('sobel_xy', sobelxy)\n",
    "cv2.imshow('scharr_xy', scharrxy)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian : 先用 Sobel 運算元計算二階 x 和 y 導數\n",
    "> Laplacianfilter是一種空間二階導數的運算子，它對於影像中`快速變化`的區域(包含edge)具有很大的強化作用\n",
    "> #### 二階 x 和 y 導數數學原理 :\n",
    "><img src=\"./image/laplacian.jpg\"  style='width:90%'></img><br>\n",
    "><img src=\"./image/laplacian.png\"  style='width:90%'></img><br>\n",
    "\n",
    "> #### 常用的 Laplacian Filters :<br>\n",
    "\n",
    ">> $\\begin{pmatrix} 0&1&0\\\\1&-4&1\\\\0&1&0 \\end{pmatrix}\n",
    " \\begin{pmatrix} 1&1&1\\\\1&-8&1\\\\1&1&1 \\end{pmatrix}\n",
    " \\begin{pmatrix} -1&2&-1\\\\2&-4&2\\\\-1&2&-1 \\end{pmatrix}$\n",
    " \n",
    "> Laplacian對於雜訊（Noise）非常敏感，因此在實用上都會將影像`先模糊化`後再處理 (LoG Laplacian of Gaussian)。<br>\n",
    "> 使用 Laplacian 找出邊緣。注意使用此函數除了傳入`灰階`影像之外，亦須指定輸出的影像浮點格式 `CV_64F`\n",
    "\n",
    "><img src=\"./image/laplacian_formula.jpg\"  style='width:100%'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "img=np.array([\n",
    "[30 ,30 ,30 ,30 ,30 ,30 ,30 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,30 ,30 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,30 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,20 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,20 ,20 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10]])/1.\n",
    "Laplacian = cv2.Laplacian(img, cv2.CV_64F)\n",
    "# Laplacian = cv2.Laplacian(img, cv2.CV_16S)  # error\n",
    "Laplacian = cv2.convertScaleAbs(Laplacian)\n",
    "Laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# o = cv2.imread('./image/sobel.bmp')\n",
    "o = cv2.imread('./image/lenaColor.png',0)\n",
    "# o=cv2.blur(o, (3,3))\n",
    "Laplacian = cv2.Laplacian(o, cv2.CV_64F, ksize=1)   # ksize=1 default\n",
    "Laplacian = cv2.convertScaleAbs(Laplacian)\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('Laplacian', Laplacian)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-3: Canny\n",
    "> 預處理圖片，轉換成`灰階`，並利用 Gaussian Blur 去除雜訊, 取得圖片每個 pixel 的`梯度值和梯度方向`, 利用非極大值抑制（Non-maximum suppression）尋找可能的邊緣\n",
    "\n",
    "> 傳入影像參數並指定兩個門檻參數 lower 與 upper，意思是，圖形的任一點像素，\n",
    "> * `若其值大於 upper，則認定它屬於邊緣像素 `\n",
    "> * `若小於 lower 則不為邊緣像素`\n",
    "> * `介於兩者之間則由程式依其像素強度值運算後決定`<br>\n",
    "\n",
    ">根據兩個閾值選取 strong edge（確定的） 和 weak edge（進一步判斷）, 選取和 strong edge 相連的 weak edge 當作確定的 edge, 。<br>\n",
    "\n",
    "> Canny 建議 upper : lower 比例 2:1 and 3:1.\n",
    "https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/canny_detector/canny_detector.html?ref=driverlayer.com/web\n",
    ">><img src=\"./image/canny.jpg\"  style='width:90%'></img>\n",
    "### 上圖中 C 要保留, B 要捨棄\n",
    "\n",
    "> Canny 邊緣檢測，其實 Canny 不能被單獨稱為一種方法，因為它是一連串的過程加上其它方法，`先模糊化`去除不必要的像素、再使用`類似 Sobel` 方式取得XY軸邊緣。它先將影像模糊化再進行非極大值抑制（non-maxima suppression），因此 Canny 比起 Sobel 較能處理雜訊問題，但是需要花費較多的硬體資源來處理。在下方的實作中我們可以看到它們輸出的差異。不過這部份技術原理已超出本人能力範圍無法深入解釋，若您對其技術原理有興趣，可再詳查其相關技術文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gaussian kernel of size = 3\n",
    ">> $ Gx = \\frac{1}{16}\\begin{pmatrix}1&2&1\\\\2&4&2\\\\1&2&1\\end{pmatrix}$\n",
    "\n",
    "> Gaussian kernel of size = 5\n",
    ">> $ Gx = \\frac{1}{159}\\begin{pmatrix}2&4&5&4&2\\\\4&9&12&9&4\\\\5&12&15&12&5\\\\4&9&12&9&4\\\\2&4&5&4&2\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ EdgeGradient (G) = \\sqrt{Gx^2 + Gy^2}$\n",
    "\n",
    "> $ Angle (θ) = tan^{−1}(\\frac{Gy}{Gx})$\n",
    "\n",
    "把當前位置的`梯度值`與`梯度方向`上兩側的梯度值進行比較\n",
    "\n",
    "梯度方向垂直於邊緣方向\n",
    "><img src=\"./image/gradientDirection.jpg\"  style='width:90%'></img><br>\n",
    "><img src=\"./image/canny_edge.png\"  style='width:90%'></img><br>\n",
    "><img src=\"./image/canny_edge1.png\"  style='width:60%'></img><br>\n",
    "https://en.wikipedia.org/wiki/Canny_edge_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ EdgeGradient(G) = \\sqrt{Gx^2 + Gy^2},   Angle (θ) = tan^{−1}(\\frac{Gy}{Gx})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "img = np.zeros((8, 8), dtype=np.uint8)\n",
    "img[1:7, 1:7] = 10  # padding problem = 10\n",
    "print(f'img :\\n{img}\\n')\n",
    "\n",
    "sobelx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=-1)  # ddepth = -1, dx=1, dy=0\n",
    "sobely = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=-1)  # ddepth = -1, dx=0, dy=1\n",
    "mag, angle = cv2.cartToPolar(sobelx, sobely, angleInDegrees=True)\n",
    "sobelxy =  cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)  \n",
    "\n",
    "print(f'sobelx :\\n{sobelx}\\n\\n'\n",
    "      f'sobely :\\n{sobely}\\n\\n'\n",
    "      f'mag :\\n{mag.round(0)}\\n\\n'\n",
    "      f'angle 0, 45, 90, 135.... :\\n{angle.round(0)}\\n\\n'\n",
    "      f'sobelxy :\\n{sobelxy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=np.array([\n",
    "[30 ,30 ,30 ,30 ,30 ,30 ,30 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,30 ,30 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,30 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,30 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,30 ,20 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10],\n",
    "[30 ,30 ,20 ,20 ,20 ,20 ,20 ,20 ,10 ,10 ,10 ,10]], dtype='uint8')\n",
    "canny = cv2.Canny(img, 15, 30)\n",
    "# Laplacian = cv2.Laplacian(img, cv2.CV_16S)  # error\n",
    "# Laplacian = cv2.convertScaleAbs(Laplacian)\n",
    "canny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canny vs. Sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "o = cv2.imread('./image/lenaColor.png', 0)\n",
    "# o = cv2.imread('./image/contour.png', 0)\n",
    "# o = cv2.imread('./image/coins.jpg',0)\n",
    "    \n",
    "r1=cv2.Canny(o, 50, 150)   # different threshold\n",
    "r2=cv2.Canny(o, 32, 96)    # different threshold\n",
    "\n",
    "sobelx = cv2.Sobel(o, cv2.CV_64F, 1, 0, ksize=3)\n",
    "sobely = cv2.Sobel(o, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "sobelx = cv2.convertScaleAbs(sobelx)   # 轉回 Uint8 \n",
    "sobely = cv2.convertScaleAbs(sobely)  \n",
    "sobelxy =  cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0) \n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('Canny r50_150', r1)\n",
    "cv2.imshow('Canny r32_96', r2)\n",
    "cv2.imshow('sobel_xy', sobelxy)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny : road edge detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('./image/road1.jpg', 1)\n",
    "image = cv2.resize(image, (750, 450), interpolation=cv2.INTER_AREA)\n",
    "# image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "cv2.imshow('Blurred', image)\n",
    "\n",
    "# Canny edge detection\n",
    "canny = cv2.Canny(image, 30, 150, apertureSize=3, L2gradient=0) # threshold1=39, threshhold2=150, sobel size=3\n",
    "cv2.imshow('Canny', canny)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video with Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "## Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv2.VideoWriter('./video/output.avi',fourcc, 20.0, (640,480))  # 20 FPS, size=640, 480\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.Canny(frame, 100, 200)\n",
    "    if ret==True:\n",
    "#         frame = cv2.flip(frame,0)\n",
    "\n",
    "# write the flipped frame\n",
    "#         out.write(frame)\n",
    "        cv2.imshow('frame', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "## Release everything if job is finished\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel vs. Laplacian vs. Canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# image = cv2.imread('./image/coins.jpg', 1)\n",
    "image = cv2.imread('./image/road1.jpg', 0)       # road edge detection\n",
    "\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "# ===== Sobel edge detection ============\n",
    "sobelX = cv2.Sobel(image, cv2.CV_64F, 1, 0)\n",
    "sobelY = cv2.Sobel(image, cv2.CV_64F, 0, 1)\n",
    "\n",
    "sobelX = cv2.convertScaleAbs(sobelX)\n",
    "sobelY = cv2.convertScaleAbs(sobelY)\n",
    "sobelXY = cv2.addWeighted(sobelX, 0.5, sobelY, 0.5, 0)\n",
    "th2, sobelXY = cv2.threshold(sobelXY, 0, 255,  cv2.THRESH_OTSU)\n",
    "\n",
    "# ===== Laplacian edge detection ========\n",
    "lap = cv2.Laplacian(image, cv2.CV_64F)\n",
    "lap = cv2.convertScaleAbs(lap)\n",
    "th2, lap = cv2.threshold(lap, 0, 255,  cv2.THRESH_OTSU)\n",
    "\n",
    "# ===== Canny edge detection ============\n",
    "canny=cv2.Canny(image, 32, 128)    # different threshold\n",
    "\n",
    "cv2.imshow('Sobel X', sobelX)\n",
    "cv2.imshow('Sobel Y', sobelY)\n",
    "cv2.imshow('Sobel XY after threshold', sobelXY)\n",
    "cv2.imshow('Laplacian after threshold', lap)\n",
    "cv2.imshow('canny', canny)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoG (Difference of Gaussian)\n",
    ">DoG (Difference of Gaussian) 是灰度圖像增強和角點檢測的方法，其做法較簡單，證明較複雜，具體講解如下:\n",
    "Difference of Gaussian(DOG) 是`高斯函數的差分`。我們已經知道可以通過將圖像與高斯函數進行卷積得到一幅圖像的低通濾波結果，即去噪過程，這裡的 Gaussian 和高斯低通濾波器的高斯一樣，是一個函數，即為常態分佈函數。<br>\n",
    "\n",
    "><img src=\"./image/Dog.png\"  style='width:90%'></img>\n",
    "><img src=\"./image/Dog.gif\"  style='width:90%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff. $\\sigma_x, \\sigma_y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('./image/lenaColor.png',0)\n",
    "# img = cv2.imread('./image/opencv.jpg',1)\n",
    "d=5\n",
    "img_D3 = cv2.GaussianBlur(img, (d, d), 0.3)\n",
    "img_D5 = cv2.GaussianBlur(img, (d, d), 0.5)   \n",
    "img_D7 = cv2.GaussianBlur(img, (d, d), 0.7)   \n",
    "img_D9 = cv2.GaussianBlur(img, (d, d), 0.9)   \n",
    "img_D11 = cv2.GaussianBlur(img, (d, d), 1.1)   \n",
    "img_D13 = cv2.GaussianBlur(img, (d, d), 1.3)   \n",
    "\n",
    "img_D5_3 = img_D5 - img_D3   # try img_G0 + img_G1\n",
    "img_D9_7 = img_D9 - img_D7   # try img_G0 + img_G1\n",
    "img_D11_13 = img_D11 - img_D13   # try img_G0 + img_G1\n",
    "\n",
    "cv2.imshow('DoG5_3', img_D5_3)\n",
    "cv2.imshow('DoG9_7', img_D9_7)\n",
    "cv2.imshow('DoG11_13', img_D11_13)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff. filter size\n",
    "> $\\sigma_x = 0.3 * [(ksize.width - 1) * 0.5-1] + 0.8$<br>\n",
    "> $\\sigma_y = 0.3 * [(ksize.width - 1) * 0.5-1] + 0.8$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "  \n",
    "# img = cv2.imread('./image/lenaColor.png')\n",
    "img = cv2.imread('./image/opencv.jpg',1)\n",
    "\n",
    "img_G0 = cv2.GaussianBlur(img, (3, 3),1)      # 𝜎𝑥 = 𝜎𝑦 = 0.8\n",
    "img_G1 = cv2.GaussianBlur(img, (5, 5),1)      # 𝜎𝑥 = 𝜎𝑦 = 1.1\n",
    "\n",
    "img_DoG = img_G0 - img_G1   # try img_G0 + img_G1\n",
    "\n",
    "cv2.imshow('DoG', img_DoG)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### img → imgG0 → img_G1 with same filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "  \n",
    "# img = cv2.imread('./image/lenaColor.png')\n",
    "img = cv2.imread('./image/opencv.jpg',1)\n",
    "\n",
    "img_G0 = cv2.GaussianBlur(img, (3, 3),1)      # img  => img_G0\n",
    "img_G1 = cv2.GaussianBlur(img_G0, (3, 3),1)   \n",
    "\n",
    "img_DoG = img_G0 - img_G1   # try img_G0 + img_G1\n",
    "\n",
    "cv2.imshow('DoG', img_DoG)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 8. 輪廓偵測 (contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 若 Edge 線條頭尾相連形成`封閉的區塊`，那麼它就是 Contour，否則就只是 Edge。Contours 是由一連串沒有間斷的點所組成的曲線，我們在針對影像進行分析及識別時，Contours 的使用是很重要的一個步驟。\n",
    "\n",
    "> `結構簡單物體`且背景色單純的圖片，我們可以直接使用灰階圖形取得該物體的 Contour，但如果是一張**複雜背景**的圖片，就需要先透過 edge detection 或 threshold 預處理才行\n",
    "\n",
    "> 一般在對圖像取 contour 前, 都會`先轉黑白`, 做 threshold, canny 等 edge detection 處理, 能提高 contour 的辨識效果. 物件必須是白色, 背景必須是黑色"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-1: cv2.findContours函數 -- 獲取輪廓\n",
    ">#### $contour, hierarhy = cv2.findContours (fgmask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)$\n",
    "> * contour 返回值 : cv2.findContours() 函式首先返回一個 list，list 中每個元素都是影像中的一個輪廓，用 numpy 中的 ndarray 表示。\n",
    "\n",
    "> * hierarchy 返回值 : 該函式還可返回一個可選的 hiararchy 結果，這是一個 ndarray，其中的元素個數和輪廓個數相同，每個輪廓 contours[i] 對應 4 個 hierarchy 元素 hierarchy[i][0] ~hierarchy[i][3]，分別表示 : \n",
    ">> * 後一個輪廓、\n",
    ">> * 前一個輪廓、\n",
    ">> * 子輪廓、\n",
    ">> * 父輪廓<br>\n",
    "\n",
    ">> 如果沒有對應項，則該值為負數。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode :\n",
    "> | parameter       |說明                                                                     |\n",
    "> |-----------------|------------------------------------------------------------------------|\n",
    "> |cv2.RETR_EXTERNAL|則表示只取外輪廓的 Contour（如果有其它 Contour 包在內部）所有孩子輪廓都不要  |\n",
    "> |cv2.RETR_LIST    |這是最簡單的一個，它獲取所有輪廓，但是不建立父子關係，他們都是一個層級。所以，層級屬性第三個和第四個欄位（父子）都是-1，但是Next和Previous還是有對應值。|\n",
    "> |cv2.RETR_TREE    |建立一個等級樹結構的輪廓                                                  |\n",
    "> |cv2.RETR_CCOMP   |建立兩個等級的輪廓，上面的一層為外邊界，裡面的一層為內孔的邊界資訊。如果內孔內還有一個連通物體，這個物體的邊界也在頂層。|\n",
    "\n",
    "---\n",
    "## Method :\n",
    "> | parameter                   |說明                                                                                            |\n",
    "> |-----------------------------|-----------------------------------------------------------------------------------------------|\n",
    "> |cv2.CHAIN_APPROX_NONE        |儲存`所有的輪廓點`，相鄰的兩個點的畫素位置差不超過1，即max（abs（x1-x2），abs（y2-y1））==1            |\n",
    "> |cv2.CHAIN_APPROX_SIMPLE      |壓縮水平方向，垂直方向，對角線方向的元素，只`保留該方向的終點座標`，例如一個矩形輪廓只需4個點來儲存輪廓資訊|\n",
    "> |cv2.CHAIN_APPROX_TC89_L1，CV_CHAIN_APPROX_TC89_KCOS  |使用 teh-Chinl chain 近似演算法                                           |\n",
    "\n",
    "> ### cv2.CHAIN_APPROX_NONE vs. cv2.CHAIN_APPROX_SIMPLE\n",
    ">><img src=\"./image/contour3.png\"  style='width:70%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-2: cv.drawContours函數 -- 繪出輪廓\n",
    ">#### cv2.drawContours(image, contours, contourIdx, color[, thickness[, lineType[, hierarchy[, maxLevel[, offset ]]]]])\n",
    "\n",
    "> * image : 是指明在哪幅影像上繪製輪廓；\n",
    "> * contours : 是輪廓本身，在Python中是一個list。\n",
    "> * contourIdx : 指定繪製輪廓list中的哪條輪廓，如果是-1，則繪製其中的所有輪廓。\n",
    "> * color : 繪製輪廓的顏色\n",
    "> * thickness : 表明輪廓線的寬度，如果是-1（cv2.FILLED），則為填充模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### cv2.RETR_EXTERNAL : 則表示只取外輪廓的 Contour（如果有其它 Contour 包在內部）所有孩子輪廓都不要\n",
    "> cv2.drawContours 可協助我們找出 Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "im = cv2.imread('./image/contour.png')\n",
    "imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "# th2, thresh = cv2.threshold(imgray, 0, 255,  cv2.THRESH_OTSU)  # type\n",
    "\n",
    "# cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "print(f'Next, Previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "img_1 = cv2.drawContours(im.copy(), cnts, -1, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "img0 = cv2.drawContours(im.copy(), cnts, 0, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "img1 = cv2.drawContours(im.copy(), cnts, 1, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "img2 = cv2.drawContours(im.copy(), cnts, 2, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "img3 = cv2.drawContours(im.copy(), cnts, 3, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "\n",
    "print (f'contours 型別\\t\\t：{type(cnts)}\\n'\n",
    "       f'第 0 個contours\\t\\t: {type(cnts[0])}\\n'\n",
    "       f'contours 數量\\t\\t：{len(cnts)}\\n')\n",
    "\n",
    "for i in range(len(cnts)):\n",
    "    print (f'contours[{i}]儲存點的個數\\t：{len(cnts[i])}')\n",
    "\n",
    "cv2.imshow('imgray', imgray)\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('img_1', img_1)\n",
    "cv2.imshow('img0', img0)\n",
    "cv2.imshow('img1', img1)\n",
    "cv2.imshow('img2', img2)\n",
    "cv2.imshow('img3', img3)   # 第三個不見了\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-3: 配合邊緣檢測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "imgray = cv2.imread('./image/contour.png', 0)\n",
    "\n",
    "ret, thresh = cv2.threshold(imgray, 225, 255, cv2.THRESH_BINARY_INV)  # try cv2.THRESH_BINARY\n",
    "\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'Next, Previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "cntsImg=[]\n",
    "for i in range(len(cnts)):\n",
    "    temp=np.zeros(imgray.shape, np.uint8)\n",
    "    cntsImg.append(temp)\n",
    "    cntsImg[i]=cv2.drawContours(cntsImg[i], cnts, i, (255,255,255), 3)\n",
    "    cv2.imshow('contours['+ str(i)+']', cntsImg[i])\n",
    "\n",
    "cv2.imshow('imgray', imgray)\n",
    "cv2.imshow('thresh', thresh)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.RETR_TREE : 建立一個等級樹結構的輪廓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "im = cv2.imread('./image/contour.png')\n",
    "imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "print(f'Next, Previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "for i in range(-1, len(cnts)) :\n",
    "    img = cv2.drawContours(im.copy(), cnts, i, (0, 255, 0), 2) # image, contour, contouridx, (color), thickness\n",
    "    cv2.imshow(f'img{i}', img)\n",
    "    if i >=0 : print (f'contours[{i}]儲存點的個數\\t：{len(cnts[i])}')\n",
    "    \n",
    "print (f'\\ncontours 型別\\t\\t：{type(cnts)}\\n'\n",
    "       f'第 0 個contours\\t\\t: {type(cnts[0])}\\n'\n",
    "       f'contours 數量\\t\\t: {len(cnts)}\\n')\n",
    "\n",
    "cv2.imshow('imgray', imgray)\n",
    "cv2.imshow('thresh', thresh)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.RETR_CCOMP : 檢索所有輪廓並將它們組織成`兩級層次結構`。上面的一層為外邊界，下面的一層為內孔的邊界。如果內孔內還有一個連通物體，那麼這個物體的邊界仍然位於頂層。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "im = cv2.imread('./image/contour.png')\n",
    "imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'Next, Previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "obj=im.copy()\n",
    "cv2.drawContours(obj, cnts, -1, (0, 255, 0), 2)  # image, contour, contouridx, (color), thickness\n",
    "cv2.imshow('imgray', imgray)\n",
    "cv2.imshow('thresh', thresh)\n",
    "cv2.imshow('Objects', obj)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boundingRect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "src = cv2.imread(\"./image/contour.png\")\n",
    "cv2.imshow(\"src\",src)\n",
    "src_gray = cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)     # 影像轉成灰階\n",
    "\n",
    "ret, dst_binary = cv2.threshold(src_gray,127,255,cv2.THRESH_BINARY)  # 二值化處理影像\n",
    "# 找尋影像內的輪廓\n",
    "contours, hierarchy = cv2.findContours(dst_binary, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)  \n",
    "lt = 16\n",
    "for i in range(len(contours)):\n",
    "    x, y, w, h = cv2.boundingRect(contours[i])        # 建構矩形\n",
    "    print(f'contour[{i}]左上角\\t\\tx = {x}\\n'\n",
    "          f'contour[{i}]左上角\\t\\ty = {y}\\n'\n",
    "          f'contour[{i}]矩形寬度\\tw = {w}\\n'\n",
    "          f'contour[{i}]矩形高度\\th = {h}\\n')\n",
    "\n",
    "    dst = cv2.rectangle(src,(x, y),(x+w, y+h),(0,0,255),2)\n",
    "    cv2.putText(dst, f'w/h : {w/h:.2f}', (x, y-5), 2, .6, (0,0,255), 1, lt)\n",
    "cv2.imshow(\"dst\",dst)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours : coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('./image/coin.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (9, 9), 0)\n",
    "\n",
    "edged = cv2.Canny(gray, 20, 40)\n",
    "# contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "out = image.copy()\n",
    "out.fill(0)\n",
    "\n",
    "cv2.drawContours(out, contours, -1, (0, 255, 255), -1)\n",
    "\n",
    "frame = cv2.hconcat([image, out])\n",
    "cv2.imshow('frame', cv2.resize(frame, None, fx=1.2, fy=1.2))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### canny & contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# image = cv2.imread('./image/mybaby.jpg')\n",
    "image = cv2.imread('./image/blox.jpg')\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "#blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "cv2.imshow('Image', image)\n",
    "\n",
    "edged = cv2.Canny(blurred, 30, 150)                       # canny\n",
    "cv2.imshow('Edges', edged)\n",
    "\n",
    "# cnts, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "print(f'count objects in this image\\t: {len(cnts)}')\n",
    "\n",
    "objs = image.copy()\n",
    "cv2.drawContours(objs, cnts, -1, (0, 255, 0), 2)          # contour\n",
    "\n",
    "cv2.imshow('Objects Contour', objs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩的計算 moment()\n",
    "> 影像識別的一個核心問題是影像的特徵提取，簡單描述即為用一組簡單的資料(資料描述量)來描述整個影像，這組資料愈簡單越有代表性越好。`良好的特徵不受光線、噪點、幾何形變的干擾`，影像識別技術的發展中，不斷有新的描述影像特徵提出，而影像不變 `矩` 就是其中一個。\n",
    "\n",
    "> 從影像中計算出來的 `矩` 通常描述了影像不同種類的幾何特徵如：`大小、灰度、方向、形狀`等，影像矩廣泛應用於模式識別、目標分類、目標識別與防偽估計、影像編碼與重構等領域。\n",
    "\n",
    "> |類別|說明|代碼|\n",
    "> |---|---|---|\n",
    "> |空間矩 | 實質為面積或者質量。可以通過一階矩計算`質心/重心`|m : m00, m10, m01, m20, m11, m02, m30, m21, m12, m03|\n",
    "> |中心矩 | 體現的是影像強度的最大和最小方向（中心矩可以構建影像的協方差矩陣），其只具有平移不變性，所以用中心矩做匹配效果不會很好|mu : mu20, mu11, mu02, mu30, mu21, mu12, mu03|\n",
    "> |歸一化中心矩 | 歸一化後具有尺度不變性|Hu nu : nu20, nu11, nu02, nu30, nu21, nu12, nu03|\n",
    "\n",
    "> https://zh.wikipedia.org/zh-tw/%E7%9F%A9_(%E5%9B%BE%E5%83%8F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "im = cv2.imread('./image/sobel2.bmp')\n",
    "cv2.imshow('original', im)\n",
    "\n",
    "imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 225, 255, cv2.THRESH_BINARY)  # try cv2.THRESH_BINARY\n",
    "\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'next, previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "cntsImg=[]\n",
    "for i in range(len(cnts)):\n",
    "    temp=np.zeros(im.shape, np.uint8)\n",
    "    cntsImg.append(temp)\n",
    "    cntsImg[i]=cv2.drawContours(cntsImg[i], cnts, i, (255,255,255), 3)\n",
    "    cv2.imshow('contours['+ str(i)+']', cntsImg[i])\n",
    "\n",
    "print('觀察各輪廓的矩(moments) :\\n')\n",
    "for i in range(len(cnts)):\n",
    "    print(f'輪廓 {i} 的矩 :\\n{cv2.moments(cnts[i])}\\n')\n",
    "    \n",
    "print('觀察各輪廓的面積 :\\n')\n",
    "for i in range(len(cnts)):\n",
    "    print(f\"輪廓 {i} 的面積\\t: {cv2.moments(cnts[i])['m00']:>10,.1f}\")\n",
    "    \n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contourArea (), arcLength(cnts[i], True)\n",
    "\n",
    "> 原始矩包含以下的一些的有關原始圖像屬性的信息：\n",
    "\n",
    "> 圖像的`幾何中心`可以表示為： ${\\displaystyle \\{{\\bar {x}},\\ {\\bar {y}}\\}=\\left\\{{\\frac {M_{10}}{M_{00}}},{\\frac {M_{01}}{M_{00}}}\\right\\}}$\n",
    "\n",
    "> 二值圖像的`面積` 或 灰度圖像的像素總和，可以表示為：${\\displaystyle M_{00}}$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "im = cv2.imread('./image/contour2.png')\n",
    "cv2.imshow('original', im)\n",
    "\n",
    "imgray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(imgray, 225, 255, cv2.THRESH_BINARY)  # try cv2.THRESH_BINARY\n",
    "\n",
    "cnts, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(f'next, previous, First_Child, Parent\\n {hierarchy}\\n')        # [next, previous, First_Child, Parent]\n",
    "\n",
    "cv2.imshow('original', im)\n",
    "\n",
    "cntsImg=[]\n",
    "for i in range(len(cnts)):\n",
    "    M = cv2.moments(cnts[i])\n",
    "    cx = int(M['m10'] / M['m00'])  # 中心點 x 座標\n",
    "    cy = int(M['m01'] / M['m00'])  # 中心點 y 座標\n",
    "    \n",
    "    area = cv2.contourArea(cnts[i])                                        # 面積\n",
    "    round_len = cv2.arcLength(cnts[i], True)                     # 週長\n",
    "    \n",
    "    print(f'輪廓 {i} 的中心點 ({cx}, {cy}),\\t面積 : {area:10,.2f},\\t週長 :{round_len:9,.2f}')\n",
    "    temp=np.zeros(im.shape, np.uint8)\n",
    "    cntsImg.append(temp)\n",
    "    cntsImg[i]=cv2.drawContours(cntsImg[i], cnts, i, (255,255,255), 3)\n",
    "\n",
    "    cv2.imshow(f'outlook {i}, area : {area}  outlook_len :{round_len}', cntsImg[i])\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 9. 形態學"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-1: 形態學介紹\n",
    ">影像處理中指的形態學，往往表示的是數學形態學。\n",
    "\n",
    ">數學形態學（Mathematical morphology） 是一門建立在格論和拓撲學基礎之上的影像分析學科，是數學形態學影像處理的基本理論。其基本的運算包括：`二值侵蝕和膨脹、二值開閉運算、骨架抽取、極限侵蝕、擊中擊不中變換、形態學梯度、Top-hat變換、顆粒分析、流域變換、灰值侵蝕和膨脹、灰值開閉運算、灰值形態學梯度等`。\n",
    "\n",
    ">簡單來講，形態學操作就是基於形狀的一系列影像處理操作。OpenCV為進行影像的形態學變換提供了快捷、方便的函式。最基本的形態學操作有二種，他們是：`侵蝕與膨脹` ( Erosion 與 Dilation )。\n",
    "\n",
    "> 侵蝕與膨脹能實現多種多樣的功能，主要如下：\n",
    "\n",
    "> * 消除噪聲\n",
    "> * 分割 ( isolate ) 出獨立的影像元素，在影像中連線(join)相鄰的元素。\n",
    "> * 尋找影像中的明顯的極大值區域或極小值區域\n",
    "> * 求出影像的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-2: 什麼是形態學\n",
    "> 形態學操作是根據影像形狀進行的簡單操作,一般情況下對`二值化影像`進行的操作。需要輸入兩個引數，一個是原始影像，第二個被稱為結構化元素或核，它是用來決定操作的性質的。<br>\n",
    "\n",
    "> 兩個基本的形態學操作是侵蝕和膨脹。他們的變體構成了開運算，閉運算，梯度等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-3: 侵蝕、膨脹、開運算、閉運算\n",
    "\n",
    ">### Erode 侵蝕 : 以 Kernel 中心點移動\n",
    "> 卷積核沿著圖像滑動，如果與卷積核對應的原圖像的`所有圖元值與 Kernel 相同, 那麼中心元素就給 1，否則就變為零`。根據卷積核的大小靠近前景的所有圖元都會被侵蝕掉（變為0），所以`前景物體會變小`，整幅圖像的白色區域會減少。這對於`去除白色雜訊很有用`，也可以用來斷開兩個連在一塊的物體等。  \n",
    "\n",
    "> cv2.erode(img, kernel=None, iterations =1)\n",
    "\n",
    "> * img : 指需要侵蝕的圖\n",
    "> * kernel : 指侵蝕操作的內核，默認是一個簡單的 3X3 全 1 的矩陣，我們也可以利用 getStructuringElement（）函數指明它的形狀\n",
    "> * iterations : 指的是侵蝕次數，省略是默認為1\n",
    "\n",
    ">在進行侵蝕和膨脹的講解之前，首先需要注意，侵蝕和膨脹是對白色部分（高亮部分）而言的，不是黑色部分。膨脹就是影像中的高亮部分進行膨脹，`領域擴張`，效果圖擁有比原圖更大的高亮區域。侵蝕就是原圖中的高亮部分被侵蝕，`領域被蠶食`，效果圖擁有比原圖更小的高亮區域\n",
    "\n",
    ">><img src=\"./image/erode.jpg\"  style='width:100%'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=np.zeros((7, 7), np.uint8)\n",
    "img[1:6, 1:6]=1\n",
    "\n",
    "kernel = np.ones((3, 3),np.uint8)  # try  [3, 3]\n",
    "erosion = cv2.erode(img, kernel, iterations = 1)     # 調整 interation 試一試\n",
    "print(f'img =\\n{img}\\n\\n'\n",
    "      f'kernel =\\n{kernel}\\n\\n'\n",
    "      f'erosion =\\n{erosion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "o=cv2.imread('./image/erode.bmp')\n",
    "kernel33 = np.ones((3, 3), np.uint8)\n",
    "erosion33 = cv2.erode(o, kernel33, iterations = 1)     # 調整 interation 試一試\n",
    "\n",
    "kernel55 = np.ones((9,9), np.uint8)\n",
    "erosion55 = cv2.erode(o, kernel55, iterations = 1)     # 調整 interation 試一試\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('erosion33', erosion33)\n",
    "cv2.imshow('erosion55', erosion55)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilate 膨脹\n",
    "> 與侵蝕相反，與卷積核對應的原圖像的圖元值中`只要有一個是 1`，中心元素的圖元值就是 1。所以這個操作會增加圖像中的白色區域（前景）。一般在去雜訊時先用侵蝕再用膨脹。因為侵蝕在去掉白色雜訊的同時，也會使前景對像變小。所以我們再對他進行膨脹。這時雜訊已經被去除了，不會再回來了，但是前景還在並會增加。\n",
    "\n",
    "> 膨脹也可以用來連接兩個分開的物體。其實，膨脹就是求區域性最大值的操作。按數學方面來說，膨脹或者侵蝕操作就是將影像（或影像的一部分割槽域，我們稱之為 A）與核（我們稱之為 B）進行卷積。\n",
    "\n",
    ">核可以是任何的形狀和大小，它擁有一個單獨定義出來的參考點，我們稱其為錨點（anchorpoint）。多數情況下，核是一個小的中間帶有參考點和實心正方形或者圓盤，其實，我們可以把核視為模板或者掩碼。\n",
    "\n",
    ">而膨脹就是求區域性最大值的操作，核B與圖形卷積，即計算核B覆蓋的區域的畫素點的最大值，並把這個最大值賦值給參考點指定的畫素。這樣就會使影像中的高亮區域逐漸增長。如下圖所示，這就是膨脹操作的初衷。<br>\n",
    ">> <img src=\"./image/dilate.jpg\"  style='width:100%'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=np.zeros((5, 5),np.uint8)\n",
    "img[2:3, 1:4]=1\n",
    "\n",
    "kernel = np.ones((3,1), np.uint8)\n",
    "dilation = cv2.dilate(img, kernel)\n",
    "\n",
    "print(f'img=\\n{img}\\n\\n'\n",
    "      f'kernel=\\n{kernel}\\n\\n'\n",
    "      f'dilation\\n{dilation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不同大小的 Kernel 膨脹 dilate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "o=cv2.imread('./image/dilation.bmp')\n",
    "\n",
    "kernel55 = np.ones((5, 5), np.uint8)\n",
    "kernel99 = np.ones((9 ,9), np.uint8)\n",
    "\n",
    "dilation55 = cv2.dilate(o, kernel55)\n",
    "dilation99 = cv2.dilate(o, kernel99)\n",
    "\n",
    "cv2.imshow('original',o)\n",
    "cv2.imshow('dilation55', dilation55)\n",
    "cv2.imshow('dilation99', dilation99)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### erode & dilate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('./image/coins.jpg')\n",
    "cv2.imshow('Original', image)\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "# cv2.imshow('Gray', gray)\n",
    "\n",
    "th, thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)  # 先二值 0, 255\n",
    "cv2.imshow(f'Thresh : {th}', thresh)\n",
    "\n",
    "# we apply erosions to reduce the size of foreground objects\n",
    "mask = cv2.erode(thresh.copy(), None, iterations=1)    # None : kernel  default is a simple 3x3 matrix\n",
    "cv2.imshow('Eroded', mask)\n",
    "\n",
    "# similarly, dilations can increase the size of the ground objects\n",
    "mask = cv2.dilate(thresh.copy(), None, iterations=1)   # None : kernel  default is a simple 3x3 matrix\n",
    "cv2.imshow('Dilated', mask)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open : MORPH_OPEN \n",
    "> 先 erode 再 dilate 叫 open 運算，作用能消除圖片上的小標點<br>\n",
    "> 降噪, 計數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img1=cv2.imread('./image/opening.bmp')\n",
    "img2=cv2.imread('./image/opening2.bmp')\n",
    "\n",
    "k=np.ones((10,10),np.uint8)\n",
    "\n",
    "r1=cv2.morphologyEx(img1, cv2.MORPH_OPEN, k)   # MORPH_OPEN\n",
    "r2=cv2.morphologyEx(img2, cv2.MORPH_OPEN, k)\n",
    "\n",
    "cv2.imshow('img1', img1)\n",
    "cv2.imshow('result1', r1)\n",
    "cv2.imshow('img2', img2)\n",
    "cv2.imshow('result2', r2)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close : MORPH_CLOSE\n",
    "> 先 dilate, 再 erode : 它經常被用來填充前景物體中的小洞，或者前景物體上的小黑點。不同前景影像連接 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread('./image/closing.bmp')\n",
    "\n",
    "k=np.ones((11, 11),np.uint8)\n",
    "\n",
    "r=cv2.morphologyEx(img, cv2.MORPH_CLOSE, k, iterations=1)\n",
    "\n",
    "cv2.imshow('img1',img)\n",
    "cv2.imshow('result1',r)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opening vs. closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('./image/opening.png', 0)\n",
    "img2 = cv2.imread('./image/closing.png', 0)\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "opening = cv2.morphologyEx(img1, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow('img1', cv2.resize(img1, (360, 240)))\n",
    "cv2.imshow('img1 Opening', cv2.resize(opening, (360, 240)))\n",
    "\n",
    "closing = cv2.morphologyEx(img2, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imshow('img2', cv2.resize(img2, (360, 240)))\n",
    "cv2.imshow('img2 Closing', cv2.resize(closing, (360, 240)))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient 梯度  (膨脹 - 侵蝕)\n",
    "> 取得前景原始影像的邊緣<br>\n",
    "> 用於獲取圖片的輪廓，形態梯度圖 = 膨脹圖 - 侵蝕圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "o=cv2.imread('./image/gradient.bmp')\n",
    "k=np.ones((5, 5), np.uint8)\n",
    "\n",
    "d = cv2.dilate(o, k)  # try to add iteration\n",
    "e = cv2.erode(o, k)\n",
    "\n",
    "r=cv2.morphologyEx(o, cv2.MORPH_GRADIENT, k)\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('erode', e)\n",
    "cv2.imshow('deilate', d)\n",
    "cv2.imshow('Gradient', r)\n",
    "cv2.imshow('d-e', d-e)\n",
    "cv2.imshow('d-o', d-o)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tophat 禮帽 ( 原圖 - Open )\n",
    "> 取得原影像雜訊資訊<br>\n",
    "> 取得比原影像邊緣更 `亮` 的邊緣<br>\n",
    "> Top Hat = 原圖 -  open 運算圖，顯示出原圖去除掉的白色部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "o=cv2.imread('./image/tophat.bmp')\n",
    "\n",
    "k=np.ones((5,5),np.uint8)\n",
    "\n",
    "r=cv2.morphologyEx(o, cv2.MORPH_TOPHAT, k)\n",
    "\n",
    "cv2.imshow('original',o)\n",
    "cv2.imshow('result',r)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackhat( close - 原圖 )\n",
    "> 取得原影內部小孔, 或前景中的小黑點<br>\n",
    "> 取得比原影像邊緣更 `暗` 的邊緣<br>\n",
    "> Black Hat = 原圖 - 閉運算， 顯示出原圖去除掉的黑色部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "o=cv2.imread('./image/blackhat.bmp')\n",
    "k=np.ones((9,9), np.uint8)\n",
    "\n",
    "r=cv2.morphologyEx(o, cv2.MORPH_BLACKHAT, k)\n",
    "\n",
    "cv2.imshow('original',o)\n",
    "cv2.imshow('result',r)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Own Kernel\n",
    "* MORPH_RECT : 矩形結構，所有元素是 1\n",
    "* MORPH_CROSS ：十字結構, 對角線元素是 1\n",
    "* MORPH_ELLIPSE : 橢圓形結構，所有元素是 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "k1 = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # same np.ones([5, 5])\n",
    "k2 = cv2.getStructuringElement(cv2.MORPH_CROSS, (5, 5))\n",
    "k3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "\n",
    "print(f'kernel1=\\n{k1}\\n\\n'\n",
    "      f'kernel2=\\n{k2}\\n\\n'\n",
    "      f'kernel3=\\n{k3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "import numpy as np\n",
    "\n",
    "o = cv2.imread('./image/kernel.bmp')\n",
    "\n",
    "k1 = cv2.getStructuringElement(cv2.MORPH_RECT, (50,50))\n",
    "k2 = cv2.getStructuringElement(cv2.MORPH_CROSS, (50,50))\n",
    "k3 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (50,50))\n",
    "\n",
    "d1 = cv2.dilate(o, k1)\n",
    "d2 = cv2.dilate(o, k2)\n",
    "d3 = cv2.dilate(o, k3)\n",
    "\n",
    "cv2.imshow('original', o)\n",
    "cv2.imshow('d1 rect', d1)\n",
    "cv2.imshow('d2 cross', d2)\n",
    "cv2.imshow('d3 ellipse', d3)\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('./image/SpongeBob.jpg')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "kernel = np.ones((3, 3), np.uint8)  # 卷積核\n",
    "\n",
    "erosion = cv2.erode(img, kernel, iterations=1)  # 腐蝕\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)  # 膨脹\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # 開運算\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)  # 閉運算\n",
    "gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)  # 形態學梯度\n",
    "tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)  # 禮帽\n",
    "blackhat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)  # 黑帽\n",
    "\n",
    "plt.figure(figsize=(12, 18))\n",
    "plt.subplot(421), plt.imshow(img), plt.title('Original')\n",
    "plt.subplot(422), plt.imshow(erosion), plt.title('Erosion')\n",
    "plt.subplot(423), plt.imshow(dilation), plt.title('Dilation')\n",
    "plt.subplot(424), plt.imshow(opening), plt.title('Opening')\n",
    "plt.subplot(425), plt.imshow(closing), plt.title('Closing')\n",
    "plt.subplot(426), plt.imshow(gradient), plt.title('Gradient')\n",
    "plt.subplot(427), plt.imshow(tophat), plt.title('Tophat')\n",
    "plt.subplot(428), plt.imshow(blackhat), plt.title('Blackhat')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary : \n",
    "| 中文名    | 英文名 | api | 原理 | 個人理解 |\n",
    "|:--------:|:------:|:---:|:----:|--------:|\n",
    "|  侵蝕    | erode             | erosion = cv2.erode(src=girl_pic, kernel=kernel) | 在窗中，只要含有０，則窗內全變為０，可以去淺色噪點\t| 淺色成分被侵蝕 |\n",
    "|  膨脹    | dilate            | dilation = cv2.dilate(src=girl_pic, kernel=kernel) | 在窗中，只要含有１，則窗內全變為１，可以增加淺色成分 | 淺色成分得膨脹 |\n",
    "| 開運算   | morphology-open   | opening = cv2.morphologyEx(girl_pic, cv2.MORPH_OPEN, kernel) | 先侵蝕，後膨脹，去白噪點 | 先合再開，對淺色成分不利 |\n",
    "| 閉運算   | morphology-close  | closing = cv2.morphologyEx(girl_pic, cv2.MORPH_CLOSE, kernel) | 先膨脹，後侵蝕，去黑噪點 | 先開再合，淺色成分得勢 |\n",
    "|形態學梯度|morphology-grandient|gradient = cv2.morphologyEx(girl_pic, cv2.MORPH_GRADIENT, kernel)|一幅影像侵蝕與膨脹的區別，可以得到輪廓|數值上解釋為：膨脹減去侵蝕|\n",
    "| 禮帽     | tophat             |\ttophat = cv2.morphologyEx(girl_pic, cv2.MORPH_TOPHAT, kernel) |\t原影像減去開運算的差 | 數值上解釋為：原影像減去開運算 |\n",
    "| 黑帽     | blackhat           | blackhat = cv2.morphologyEx(girl_pic, cv2.MORPH_BLACKHAT, kernel) | 閉運算減去原影像的差 | 數值上解釋為：閉運算減去原影像 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>\n",
    "\n",
    "# Module 10. 影像模板匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-1: 什麼是影像模板匹配\n",
    "> 模板匹配是一種最原始、最基本、最常用的模式識別方法，研究某一特定物件物的圖案位於圖像的什麼地方，進而識別物件物，這就是一個匹配問題。\n",
    "\n",
    "> 模板就是一副已知的小圖像，而模板匹配就是在一副大圖像中搜尋目標，已知該圖中有要找的目標，且該目標同模板`有相同的尺寸、方向和圖像元素`，通過一定的演算法可以在圖中找到目標，確定其座標位置。\n",
    "\n",
    "><img src=\"./image/match.png\"  style='width:80%'></img>\n",
    "\n",
    "> 這就是說，我們要找的模板是圖像裡`標標準準存在的`，這裡說的標標準準，就是說，一旦圖像或者模板發生變化，比如旋轉，修改某幾個圖元，圖像翻轉等操作之後，我們就無法進行匹配了，這也是這個演算法的弊端。所以這種匹配演算法，相當於`人工智障式`匹配，就是在待檢測圖像上，從左到右，從上向下對模板圖像與小東西的圖像進行比對。\n",
    "\n",
    ">模板匹配，是一種在給定的目標影像中尋找給定的模板影像的技術，原理很簡單，就是利用一些計算相似度的公式來判斷兩張影像之間有多相似\n",
    "\n",
    "><img src=\"./image/lenaMatch.jpg\"  style='width:50%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-2: 影像模板匹配介紹\n",
    "\n",
    "> 模板影像小於目標影像的話，就需要用 sliding window 的方式來得到多個匹配的結果，可以選擇取最佳匹配或是設定一個門檻值，只要比這個門檻值好的結果都認為是有效的匹配<br>\n",
    "                                                                                             \n",
    ">### 距離 distance / 相似度 similarity\n",
    "# 距離的定義\n",
    "1. Euclidean distance(歐基里德距離)\n",
    "    - $d(i, j) = \\sqrt{|x_{i1} - x_{j1}|^2 + |x_{i2} - x_{j2}|^2 + \\dots + |x_{in} - x_{jn}|^2}$\n",
    "\n",
    "2. Cosine Similarity: 衡量cosine(theta)的大小\n",
    "$$\\cos{\\theta} = \\frac{A \\cdot B}{\\| {A} \\|_2 \\| {B} \\|_2}$$\n",
    "    - if $A = [1,2,0,4]$ and $B = [3,2,1,0]$\n",
    "    - $\\cos{\\theta} = \\frac{1 \\cdot 3 + 2 \\cdot 2 + 0 \\cdot 1 + 4 \\cdot 0} {\\sqrt{1^2+2^2+0^2+4^2} \\cdot \\sqrt{3^2+2^2+1^2+0^2}}$\n",
    "\n",
    ">><img src=\"./image/distance.jpg\"  style='width:80%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">><img src=\"./image/vector.png\"  style='width:70%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# two points\n",
    "# a = np.array((1,0));   b = np.array((0,1))\n",
    "a = np.array((2, 3, 6, 9, 5));   b = np.array((5, 7, 1, 2, 2))\n",
    "\n",
    "# distance b/w a and b\n",
    "np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine / Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "a00 = distance.cosine([1, 0], [1, 0])\n",
    "a45 = distance.cosine([1, 0], [1, 1])\n",
    "a90 = distance.cosine([1, 0], [0, 1])\n",
    "a135 = distance.cosine([1, 0], [-1, 1])\n",
    "a180 = distance.cosine([1, 0], [-1, 0])\n",
    "a225 = distance.cosine([1, 0], [-1, -1])\n",
    "a270 = distance.cosine([1, 0], [0, -1])\n",
    "a315 = distance.cosine([1, 0], [1, -1])\n",
    "\n",
    "# e = distance.cosine([1, 0, 0], [0, 0, 1])     # 角度概念 3 elements\n",
    "# f = distance.cosine([100, 0, 0], [0, 1, 0])\n",
    "# g = distance.cosine([1, 1, 0, 0], [0, 1, 0, 5]) # 4 elements\n",
    "\n",
    "print(f'a00 distance \\t= {a00:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[1, 0]])}\\n'\n",
    "      f'a45 distance \\t= {a45:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[1, 1]])}\\n'\n",
    "      f'a90 distance \\t= {a90:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[0, 1]])}\\n'\n",
    "      f'a135 distance \\t= {a135:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[-1, 1]])}\\n'\n",
    "      f'a180 distance \\t= {a180:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[-1, 0]])}\\n'\n",
    "      f'a225 distance \\t= {a225:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[-1, -1]])}\\n'\n",
    "      f'a270 distance \\t= {a270:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[0, -1]])}\\n'\n",
    "      f'a315 distance \\t= {a315:.2f},\\t\\tsimilarity = {cosine_similarity([[1, 0]], [[1, -1]])}\\n'\n",
    "      f'{cosine_similarity([[1,0,0]], [[0,0,1]])}')  # similarity is opposite with distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "a=distance.hamming([1, 0, 0], [0, 1, 0])\n",
    "b=distance.hamming([1, 0, 0], [1, 0, 0])\n",
    "c=distance.hamming([1, 0, 0], [2, 0, 0])\n",
    "d=distance.hamming([1, 0, 0], [3, 0, 0])\n",
    "a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Square difference 平方差, 這是最常見的數學公式\n",
    "\n",
    "> * Correlation 相關性 : 計算 dot product (內積)，可以想成是計算兩個向量在空間中的距離有多近，就是用 cosine 去算夾角，cosine `值越大代表夾角越小`，代表越接近 ( 假設模板影像是 10*10 的影像，可以被看作是 100 維的向量，每一維是像素的值 )<br>\n",
    "\n",
    ">><img src=\"./image/Dot-Product.jpg\"  style='width:100%'></img>\n",
    "\n",
    ">> a dot b 越大, $cos \\theta$ 夾角越小\n",
    "\n",
    ">><img src=\"./image/cosine.png\"  style='width:90%'></img>\n",
    "\n",
    ">> Correlation coefficient 與 Correlation 差別只在於計算內積時還要減去各個向量的平均值，如此一來相關性就會被放大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-3: 實作影像模板匹配\n",
    "|說明        |語法 method  | 代碼  |值  |\n",
    "|:-------------|:---------------|:-:|---|\n",
    "|平方差匹配     |TM_SQDIFF        |1 |minVal|\n",
    "|標準平方差匹配 |TM_SQDIFF_NORMED |2 |minVal|\n",
    "|相關匹配       |TM_CCORR         |3 |maxVal|\n",
    "|標準相關匹配   |TM_CCORR_NORMED  |4 |maxVal|\n",
    "|相關匹配       |TM_CCOEFF        |5 |maxVal|\n",
    "|標準相關匹配   |TM_CCOEFF_NORMED |6 |-1 ~ 1 (maxVal)|\n",
    "\n",
    "> ### TM_SQDIFF 平方差匹配 : `minVal`\n",
    "\n",
    ">> 從名字來理解，平方差匹配就是通過計算每個圖元點的`差的平方的和`，和數學中統計裡面的`平方差類似`。但是因為我們要的只是一個值，所以我們最後不需要求平均。<br>\n",
    "\n",
    ">> $R(x,y)= \\sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2$\n",
    "\n",
    "> ### TM_SQDIFF_NORMED 標準平方差匹配 : `minVal`\n",
    ">> 這個只是對上面的進行了標準化處理，經過處理後，上面的值`就不會太大`<br>\n",
    "\n",
    ">>$R(x,y)= \\frac{\\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# methods=[cv.TM_SQDIFF_NORMED,cv.TM_CCORR_NORMED,cv.TM_CCOEFF_NORMED]\n",
    "\n",
    "img=cv2.imread('./image/lenaColor.png')\n",
    "template = img[220:300, 240:360]   # 模板\n",
    "# template = img[320:400, 250:300]\n",
    "cv2.imshow('template', template)\n",
    "\n",
    "th, tw, _ = template.shape[::]    # 模板大小\n",
    "# rv = cv2.matchTemplate(img, template, cv2.TM_SQDIFF_NORMED)\n",
    "rv = cv2.matchTemplate(img, template, cv2.TM_SQDIFF)\n",
    "\n",
    "minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(rv)\n",
    "\n",
    "topLeft = minLoc\n",
    "bottomRight = (topLeft[0]+tw, topLeft[1]+th)\n",
    "\n",
    "cv2.rectangle(img, topLeft, bottomRight, 255, 2)\n",
    "print(minVal)\n",
    "fig=plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121);  plt.imshow(rv, cmap='gray');  plt.title('SQDIFF_NORMED, minVal')\n",
    "plt.subplot(122);  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), cmap='gray');  plt.title('result')\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### TM_CCORR 相關匹配 : `maxVal`\n",
    ">> 這類方法採用範本和圖像間的乘法操作, 所以``較大的數表示匹配程度較高``, 0標識最壞的匹配效果。<br>\n",
    "\n",
    ">> $R(x,y)= \\sum _{x',y'} (T(x',y') \\cdot I(x+x',y+y'))$\n",
    "> ### TM_CCORR_NORMED 標準相關匹配 : `maxVal`\n",
    ">> 這個只是對上面的進行了標準化處理，經過處理後，上面的`值就不會太大`。<br>\n",
    "\n",
    ">> $R(x,y)= \\frac{\\sum_{x',y'} (T(x',y') \\cdot I(x+x',y+y'))}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# methods=[cv.TM_SQDIFF_NORMED,cv.TM_CCORR_NORMED,cv.TM_CCOEFF_NORMED]\n",
    "\n",
    "img=cv2.imread('./image/lenaColor.png', 1)\n",
    "template = img[220:300, 240:360]\n",
    "# template = img[320:400, 250:300]\n",
    "cv2.imshow('template',template)\n",
    "\n",
    "th, tw, _ = template.shape[::]\n",
    "rv = cv2.matchTemplate(img, template, cv2.TM_CCORR_NORMED)\n",
    "\n",
    "minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(rv)\n",
    "topLeft = maxLoc\n",
    "bottomRight = (topLeft[0]+tw, topLeft[1]+th)\n",
    "\n",
    "cv2.rectangle(img, topLeft, bottomRight, 255, 2)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121), plt.imshow(rv, cmap='gray');  plt.title('CCORR_NORMED, MaxVal')\n",
    "plt.subplot(122), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), cmap='gray');  plt.title('result')\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TM_CCOEFF 相關匹配 : maxVal\n",
    "> 這類方法將模版對其均值的相對值與圖像對其均值的相關值進行匹配, 越大越相似<br>\n",
    ">>$R(x,y)= \\sum _{x',y'} (T'(x',y') \\cdot I'(x+x',y+y'))$<br>\n",
    "\n",
    ">> where<br>\n",
    "\n",
    ">>$\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum \n",
    "_{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \n",
    "\\cdot \\sum _{x'',y''} I(x+x'',y+y'') \\end{array}$\n",
    "\n",
    "### cv2.TM_CCOEFF_NORMED 標準相關匹配 : maxVal\n",
    "> 這個只是對上面的進行了標準化處理，經過處理後，上面的值就不會太大。\n",
    "> 計算出的相關係數被限制`在了 -1 到 1 之間`\n",
    "> * 1 表示完全相同\n",
    "> * -1 表示亮度正好相反\n",
    "> * 0 表示没有線性相關隨機序列\n",
    "\n",
    ">> $R(x,y)= \\frac{ \\sum_{x',y'} (T'(x',y') \\cdot I'(x+x',y+y')) }{ \\sqrt{\\sum_{x',y'}T'(x',y')^2 \\cdot \\sum_{x',y'} I'(x+x',y+y')^2} }$<br>\n",
    "\n",
    ">> where <br>\n",
    "\n",
    ">> $\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum \n",
    "_{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \n",
    "\\cdot \\sum _{x'',y''} I(x+x'',y+y'') \\end{array}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# methods=[cv.TM_SQDIFF_NORMED,cv.TM_CCORR_NORMED,cv.TM_CCOEFF_NORMED]\n",
    "\n",
    "img=cv2.imread('./image/lenaColor.png', 1)\n",
    "template = img[220:300, 240:360]\n",
    "# template = img[320:400, 250:300]\n",
    "cv2.imshow('template',template)\n",
    "th, tw, _ = template.shape[::]\n",
    "rv=cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(rv)\n",
    "print(maxVal)\n",
    "topLeft = maxLoc                                   # 只拿 max 來用\n",
    "bottomRight = (topLeft[0]+tw, topLeft[1]+th)\n",
    "\n",
    "cv2.rectangle(img, topLeft, bottomRight, 255, 2)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121), plt.imshow(rv, cmap='gray');  plt.title('CCOEFF, MaxVal')\n",
    "plt.subplot(122), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[3,6,8,1,2,88],\n",
    "            [6,3,9,14,6,22]])  # return index\n",
    "\n",
    "b=np.where(a>5, 1, -1)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多對向的模板匹配\n",
    "### use np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# methods=[cv.TM_SQDIFF_NORMED,cv.TM_CCORR_NORMED,cv.TM_CCOEFF_NORMED]\n",
    "\n",
    "img=cv2.imread('./image/lenaColor4.jpg', 1)\n",
    "template = img[200:250, 200:300]\n",
    "# template = img[320:400, 250:300]\n",
    "cv2.imshow('template', template)    \n",
    "\n",
    "th, tw, _ = template.shape[::]\n",
    "rv = cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)   # 在 -1 到 1 之間\n",
    "# print(rv)\n",
    "#----- threshold -------\n",
    "threshold = .99   # try 0.95\n",
    "loc=np.where(rv >= threshold)        # 大於 threshold 的 index 都撈出來\n",
    "print(f'numbers of match rectangle : {len(loc[0])}\\n')\n",
    "\n",
    "for pt in zip(*loc[::]):\n",
    "#     print(f'match rectangle top_left : {pt}')\n",
    "    cv2.rectangle(img, pt, (pt[0]+tw, pt[1]+th), 255, 1)\n",
    "    \n",
    "plt.figure(figsize = (8, 8))    \n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "img_rgb = cv2.imread('./image/mario.jpg')\n",
    "# img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "template = cv2.imread('./image/marioCoin.jpg', 1)\n",
    "template.shape\n",
    "\n",
    "h, w, _ = template.shape[::]\n",
    "\n",
    "res = cv2.matchTemplate(img_rgb, template, cv2.TM_CCOEFF_NORMED)    # 在了 -1 到 1 之間\n",
    "# print(res)\n",
    "threshold = .99\n",
    "loc = np.where(res >= threshold)\n",
    "print(f'numbers of match rectangle : {len(loc[0])}\\n')\n",
    "\n",
    "for idx, pt in enumerate(zip(*loc[::-1])):\n",
    "#     print(f'match rectangle top_left : {pt}')\n",
    "    cv2.rectangle(img_rgb, pt, (pt[0]+w, pt[1]+h), (0, 255, 255), 2)\n",
    "    \n",
    "# cv2.imwrite('res.png',img_rgb)\n",
    "cv2.imshow('mario coin', template)\n",
    "cv2.imshow('result', img_rgb)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>--- The End ---</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
